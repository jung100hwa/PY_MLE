# -*- coding: utf-8 -*-
"""벡터 데이터베이스 (크로마와 파이스) - wikidocs - 25-06-26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lnrjk8dkiFn3yF6_BQCx2cwQPdUQjAY7
"""

!pip install langchain-community pypdf chromadb faiss-cpu

import os
import urllib.request
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.vectorstores import FAISS

os.environ['OPENAI_API_KEY'] = "여러분의 키 값"

urllib.request.urlretrieve("https://github.com/chatgpt-kr/openai-api-tutorial/raw/main/ch06/2023_%EB%B6%81%ED%95%9C%EC%9D%B8%EA%B6%8C%EB%B3%B4%EA%B3%A0%EC%84%9C.pdf", filename="2023_북한인권보고서.pdf")

loader = PyPDFLoader('2023_북한인권보고서.pdf')
pages = loader.load_and_split()
print('청크의 수:', len(pages))

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

splitted_docs = text_splitter.split_documents(pages)
print('분할된 청크의 수:', len(splitted_docs))

chunks = [splitted_doc.page_content for splitted_doc in splitted_docs]
print('청크의 최대 길이 :',max(len(chunk) for chunk in chunks))
print('청크의 최소 길이 :',min(len(chunk) for chunk in chunks))
print('청크의 평균 길이 :',sum(map(len, chunks))/len(chunks))

db = Chroma.from_documents(splitted_docs, OpenAIEmbeddings(chunk_size=100))
print('문서의 수:', db._collection.count())

question = '북한의 교육과정'
docs = db.similarity_search(question)
print('문서의 수:', len(docs))

for doc in docs:
  print(doc)
  print('--' * 10)

db_to_file = Chroma.from_documents(splitted_docs, OpenAIEmbeddings(chunk_size=100), persist_directory = './chroma_test.db')
print('문서의 수:', db_to_file._collection.count())

db_from_file = Chroma(persist_directory='./chroma_test.db',
              embedding_function=OpenAIEmbeddings())
print('문서의 수:', db_from_file._collection.count())

question = '북한의 교육 과정'
top_three_docs = db_from_file.similarity_search_with_relevance_scores(question, k=3)

for doc in top_three_docs:
  print(doc)
  print('--' * 10)

faiss_db = FAISS.from_documents(splitted_docs, OpenAIEmbeddings(chunk_size=100))
print('문서의 수:', faiss_db.index.ntotal)

faiss_db.save_local('faiss_index')

new_db_faiss = FAISS.load_local('faiss_index',
                OpenAIEmbeddings(),
                allow_dangerous_deserialization=True)

question = '북한의 교육 과정'
docs = new_db_faiss.similarity_search(question)

for doc in docs:
  print(doc)
  print('--' * 10)