{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e94527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user06\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForMaskedLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e2566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 버트는 사전훈련된 모델이다. 원하는 종류의 모델 이름으로 분류해서 로드한다.\n",
    "# 또한 그 이름에 맞는 즉 그 모델에 맞는 또는 그 모델을 만들때 사용했던 토크나이저를 로드한다.\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-large-uncased', from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6b096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[ 101, 4715, 2003, 1037, 2428, 4569,  103, 1012,  102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
      "tf.Tensor([[ 101 4715 2003 1037 2428 4569  103 1012  102]], shape=(1, 9), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 주어진 문장에 대한 인코딩 테스트\n",
    "inputs = tokenizer('Soccer is a really fun [MASK].', return_tensors='tf')\n",
    "print(inputs) # 어떤 형태인지 볼려고 한번 찍어봄\n",
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea91c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1 1 1 1 1 1 1 1 1]], shape=(1, 9), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 확인. 현재는 패딩을 진행하지 않았기 때문에 전부 1이겠지\n",
    "print(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e90c8e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "# 버트 역시 트랜스포머 기반이다보니 transformes 패키지에는 다양한 기능을 가긴 라이브러리가 포함되어 있다\n",
    "# FillMaskPipeline 역시 모델과 토큰나이저를 지정하면 mask값을 구해준다.\n",
    "\n",
    "from transformers import FillMaskPipeline\n",
    "pip = FillMaskPipeline(model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d7ef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7621216177940369,\n",
       "  'token': 4368,\n",
       "  'token_str': 'sport',\n",
       "  'sequence': 'soccer is a really fun sport.'},\n",
       " {'score': 0.20342311263084412,\n",
       "  'token': 2208,\n",
       "  'token_str': 'game',\n",
       "  'sequence': 'soccer is a really fun game.'},\n",
       " {'score': 0.01220872811973095,\n",
       "  'token': 2518,\n",
       "  'token_str': 'thing',\n",
       "  'sequence': 'soccer is a really fun thing.'},\n",
       " {'score': 0.001863053534179926,\n",
       "  'token': 4023,\n",
       "  'token_str': 'activity',\n",
       "  'sequence': 'soccer is a really fun activity.'},\n",
       " {'score': 0.0013355057453736663,\n",
       "  'token': 2492,\n",
       "  'token_str': 'field',\n",
       "  'sequence': 'soccer is a really fun field.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask에 올 단어를 상위 5개를 보여준다.\n",
    "pip('Soccer is a really fun [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67999df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.25629115104675293,\n",
       "  'token': 2265,\n",
       "  'token_str': 'show',\n",
       "  'sequence': 'the avengers is a really fun show.'},\n",
       " {'score': 0.1728418618440628,\n",
       "  'token': 3185,\n",
       "  'token_str': 'movie',\n",
       "  'sequence': 'the avengers is a really fun movie.'},\n",
       " {'score': 0.11107777059078217,\n",
       "  'token': 2466,\n",
       "  'token_str': 'story',\n",
       "  'sequence': 'the avengers is a really fun story.'},\n",
       " {'score': 0.07249009609222412,\n",
       "  'token': 2186,\n",
       "  'token_str': 'series',\n",
       "  'sequence': 'the avengers is a really fun series.'},\n",
       " {'score': 0.07046662271022797,\n",
       "  'token': 2143,\n",
       "  'token_str': 'film',\n",
       "  'sequence': 'the avengers is a really fun film.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('The Avengers is a really fun [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "092bfae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.35730797052383423,\n",
       "  'token': 2147,\n",
       "  'token_str': 'work',\n",
       "  'sequence': 'i went to work this morning.'},\n",
       " {'score': 0.2330450713634491,\n",
       "  'token': 2793,\n",
       "  'token_str': 'bed',\n",
       "  'sequence': 'i went to bed this morning.'},\n",
       " {'score': 0.1284506618976593,\n",
       "  'token': 2082,\n",
       "  'token_str': 'school',\n",
       "  'sequence': 'i went to school this morning.'},\n",
       " {'score': 0.06230601668357849,\n",
       "  'token': 3637,\n",
       "  'token_str': 'sleep',\n",
       "  'sequence': 'i went to sleep this morning.'},\n",
       " {'score': 0.046952664852142334,\n",
       "  'token': 2465,\n",
       "  'token_str': 'class',\n",
       "  'sequence': 'i went to class this morning.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('I went to [MASK] this morning.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
