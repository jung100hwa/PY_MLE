{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fc3aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user01\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from transformers import AutoTokenizer, TFGPT2Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9c715f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xnli.test.ko.tsv', <http.client.HTTPMessage at 0x28acbf76e40>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/multinli.train.ko.tsv\", filename=\"multinli.train.ko.tsv\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/snli_1.0_train.ko.tsv\", filename=\"snli_1.0_train.ko.tsv\")\n",
    "\n",
    "# 검증 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.dev.ko.tsv\", filename=\"xnli.dev.ko.tsv\")\n",
    "\n",
    "# 테스트 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.test.ko.tsv\", filename=\"xnli.test.ko.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac930d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snli = pd.read_csv(\"snli_1.0_train.ko.tsv\", sep='\\t', quoting=3)\n",
    "train_xnli = pd.read_csv(\"multinli.train.ko.tsv\", sep='\\t', quoting=3)\n",
    "val_data = pd.read_csv(\"xnli.dev.ko.tsv\", sep='\\t', quoting=3)\n",
    "test_data = pd.read_csv(\"xnli.test.ko.tsv\", sep='\\t', quoting=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e16cf62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>개념적으로 크림 스키밍은 제품과 지리라는 두 가지 기본 차원을 가지고 있다.</td>\n",
       "      <td>제품과 지리학은 크림 스키밍을 작동시키는 것이다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>시즌 중에 알고 있는 거 알아? 네 레벨에서 다음 레벨로 잃어버리는 거야 브레이브스...</td>\n",
       "      <td>사람들이 기억하면 다음 수준으로 물건을 잃는다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>우리 번호 중 하나가 당신의 지시를 세밀하게 수행할 것이다.</td>\n",
       "      <td>우리 팀의 일원이 당신의 명령을 엄청나게 정확하게 실행할 것이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어떻게 아세요? 이 모든 것이 다시 그들의 정보다.</td>\n",
       "      <td>이 정보는 그들의 것이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그래, 만약 네가 테니스화 몇 개를 사러 간다면, 나는 왜 그들이 100달러대에서 ...</td>\n",
       "      <td>테니스화의 가격은 다양하다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0         개념적으로 크림 스키밍은 제품과 지리라는 두 가지 기본 차원을 가지고 있다.   \n",
       "1  시즌 중에 알고 있는 거 알아? 네 레벨에서 다음 레벨로 잃어버리는 거야 브레이브스...   \n",
       "2                  우리 번호 중 하나가 당신의 지시를 세밀하게 수행할 것이다.   \n",
       "3                       어떻게 아세요? 이 모든 것이 다시 그들의 정보다.   \n",
       "4  그래, 만약 네가 테니스화 몇 개를 사러 간다면, 나는 왜 그들이 100달러대에서 ...   \n",
       "\n",
       "                              sentence2  gold_label  \n",
       "0           제품과 지리학은 크림 스키밍을 작동시키는 것이다.     neutral  \n",
       "1            사람들이 기억하면 다음 수준으로 물건을 잃는다.  entailment  \n",
       "2  우리 팀의 일원이 당신의 명령을 엄청나게 정확하게 실행할 것이다.  entailment  \n",
       "3                        이 정보는 그들의 것이다.  entailment  \n",
       "4                       테니스화의 가격은 다양하다.     neutral  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xnli[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a481f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결합 후 섞기\n",
    "# train_data = train_snli.append(train_xnli)\n",
    "train_data = pd.concat([train_snli, train_xnli])\n",
    "train_data = train_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f53a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248002</th>\n",
       "      <td>8명의 소녀들이 노란색 유니폼을 입고 밖에서 치어리딩을 하고 있다.</td>\n",
       "      <td>이 소녀들은 각각 휠체어를 탄다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>파란색과 흰색 옷을 입은 축구선수가 팀 동료에게 공을 차주고 있고 다른 팀 동료들과...</td>\n",
       "      <td>공은 테니스 공이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308490</th>\n",
       "      <td>페너는 턱수염을 쪼개며 활짝 웃으며 소리쳤다. \"저 해안은 경주였어!</td>\n",
       "      <td>페너는 인상을 찌푸리며 \"그건 지루한 경주였다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>난로 위에서 음식 냄비를 휘젓는 키 큰 남자.</td>\n",
       "      <td>난로에서 요리하는 키 큰 남자.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156467</th>\n",
       "      <td>하지만 보통 9월 말까지 모든 것이 이루어진다는 것은 기본적으로 어떤 것도 재배하는...</td>\n",
       "      <td>9월 말 이후에는 아무것도 수확되지 않는다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence1  \\\n",
       "248002              8명의 소녀들이 노란색 유니폼을 입고 밖에서 치어리딩을 하고 있다.   \n",
       "7375    파란색과 흰색 옷을 입은 축구선수가 팀 동료에게 공을 차주고 있고 다른 팀 동료들과...   \n",
       "308490             페너는 턱수염을 쪼개며 활짝 웃으며 소리쳤다. \"저 해안은 경주였어!   \n",
       "6721                            난로 위에서 음식 냄비를 휘젓는 키 큰 남자.   \n",
       "156467  하지만 보통 9월 말까지 모든 것이 이루어진다는 것은 기본적으로 어떤 것도 재배하는...   \n",
       "\n",
       "                         sentence2     gold_label  \n",
       "248002          이 소녀들은 각각 휠체어를 탄다.  contradiction  \n",
       "7375                   공은 테니스 공이다.  contradiction  \n",
       "308490  페너는 인상을 찌푸리며 \"그건 지루한 경주였다.  contradiction  \n",
       "6721             난로에서 요리하는 키 큰 남자.     entailment  \n",
       "156467    9월 말 이후에는 아무것도 수확되지 않는다.        neutral  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a01b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na_and_duplciates(df):\n",
    "  df = df.dropna()\n",
    "  df = df.drop_duplicates()\n",
    "  df = df.reset_index(drop=True)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f15ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 및 중복 샘플 제거\n",
    "train_data = drop_na_and_duplciates(train_data)\n",
    "val_data = drop_na_and_duplciates(val_data)\n",
    "test_data = drop_na_and_duplciates(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97810b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 샘플 개수 : 941814\n",
      "검증용 샘플 개수 : 2490\n",
      "테스트용 샘플 개수 : 5010\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 샘플 개수 :',len(train_data))\n",
    "print('검증용 샘플 개수 :',len(val_data))\n",
    "print('테스트용 샘플 개수 :',len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9ed50",
   "metadata": {},
   "source": [
    "## GPT 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c231a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='<s>', eos_token='</s>', pad_token='<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fd58fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "</s>\n",
      "<usr>\n",
      "<pad>\n",
      "<sys>\n"
     ]
    }
   ],
   "source": [
    "# 정해진 정수값 확인\n",
    "print(tokenizer.decode(0))\n",
    "print(tokenizer.decode(1))\n",
    "print(tokenizer.decode(2))\n",
    "print(tokenizer.decode(3))\n",
    "print(tokenizer.decode(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f12aec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장1 : 모든 섬사람들이 알고 있듯이, 가장 멋진 만과 해변들 중 많은 것들은 보트로만 도달할 수 있다.\n",
      "문장2 : 보트는 일부 지역으로 가는 유일한 여행 수단이다.\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 128\n",
    "\n",
    "sent1 = '모든 섬사람들이 알고 있듯이, 가장 멋진 만과 해변들 중 많은 것들은 보트로만 도달할 수 있다.'\n",
    "sent2 = '보트는 일부 지역으로 가는 유일한 여행 수단이다.'\n",
    "\n",
    "print('문장1 :',sent1)\n",
    "print('문장2 :',sent2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0853e1",
   "metadata": {},
   "source": [
    "위의 두 개의 문장으로부터 KoGPT-2에 넣을 입력으로 전처리를 진행해봅시다. KoGPT-2가 두 개의 서로 다른 문장임을 인식할 수 있도록 힌트를 줄 필요가 있습니다. 저자는 각 문장의 시작과 끝에 KoGPT-2의 시작 토큰과 종료 토큰을 붙이는 방식을 택했습니다. 그리고 입력이 완전히 끝났다는 것을 알려주기 위해서 <unused0>라는 사용 용도가 정해져 있지 않은 KoGPT-2의 스페셜 토큰을 사용하였습니다. 그 후 배치 연산을 위해 패딩을 해줍니다. KoGPT-2의 패딩 토큰은 정수 3입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4fdfc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 전: ['<s>', '▁모든', '▁섬', '사람들이', '▁알고', '▁있듯이,', '▁가장', '▁멋진', '▁만과', '▁해변', '들', '▁중', '▁많은', '▁것들은', '▁보', '트로', '만', '▁도달할', '▁수', '▁있다.', '</s>', '<s>', '▁보', '트는', '▁일부', '▁지역으로', '▁가는', '▁유일한', '▁여행', '▁수단이', '다.', '</s>', '<unused0>']\n",
      "정수 인코딩 후: [0, 9548, 9709, 34539, 12487, 42370, 9278, 43719, 34766, 23545, 7285, 9044, 9366, 24860, 9049, 11714, 7489, 48699, 9025, 10960, 1, 0, 9049, 11943, 9616, 14303, 11318, 13382, 12079, 26626, 9016, 1, 9]\n",
      "패딩 후: [    0  9548  9709 34539 12487 42370  9278 43719 34766 23545  7285  9044\n",
      "  9366 24860  9049 11714  7489 48699  9025 10960     1     0  9049 11943\n",
      "  9616 14303 11318 13382 12079 26626  9016     1     9     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3]\n"
     ]
    }
   ],
   "source": [
    "bos_token = [tokenizer.bos_token]\n",
    "eos_token = [tokenizer.eos_token]\n",
    "\n",
    "# 첫번째 문장의 앞과 뒤에 시작 토큰 <s>과 종료 토큰 </s>으로 감싼다.\n",
    "sent1_tokens = bos_token + tokenizer.tokenize(sent1) + eos_token\n",
    "\n",
    "# 두번째 문장의 앞과 뒤에 시작 토큰 <s>과 종료 토큰 </s>으로 감싼다. 그 후 <unused0>를 붙인다.\n",
    "sent2_tokens = bos_token + tokenizer.tokenize(sent2) + eos_token + ['<unused0>']\n",
    "\n",
    "# 두 개의 문장을 연달아 이어붙인 후 정수 인코딩을 수행한다.\n",
    "tokens = sent1_tokens + sent2_tokens\n",
    "input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('정수 인코딩 전:', tokens)\n",
    "print('정수 인코딩 후:', input_id)\n",
    "\n",
    "# 최대 길이로 패딩\n",
    "input_id = pad_sequences([input_id], maxlen=max_seq_len, value=tokenizer.pad_token_id, padding='post')[0]\n",
    "print('패딩 후:', input_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d7d7d",
   "metadata": {},
   "source": [
    "전처리 결과는 위와 같습니다. 각 문장의 시작에는 시작 토큰인 <s>가 붙어있으며 정수로는 0입니다. 각 문장의 끝에는 종료 토큰인 </s>가 붙어있으며 정수로는 1입니다. 이에 따라 두 개의 문장의 앞, 뒤에는 0과 1이 있습니다. 그리고 입력이 완전히 끝나면 <unused0>에 해당하는 정수인 9가 부착됩니다. 그리고 최대 길이를 128로 정하였으므로 128의 길이로 일치시켜주기 위해서 패딩 토큰인 정수 3이 채워집니다.\n",
    "\n",
    "위 과정을 convert_examples_to_features 라는 함수로 만들고, 훈련 데이터의 첫번째 샘플을 가지고 임의로 진행했던 전처리를 훈련 데이터, 검증 데이터, 테스트 데이터에 대해서 모두 진행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83cab4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(sent_list1, sent_list2, max_seq_len, tokenizer):\n",
    "\n",
    "    input_ids = []\n",
    "\n",
    "    for sent1, sent2 in tqdm(zip(sent_list1, sent_list2), total=len(sent_list1)):\n",
    "        bos_token = [tokenizer.bos_token]\n",
    "        eos_token = [tokenizer.eos_token]\n",
    "        sent1_tokens = bos_token + tokenizer.tokenize(sent1) + eos_token\n",
    "        sent2_tokens = bos_token + tokenizer.tokenize(sent2) + eos_token + ['<unused0>']\n",
    "        tokens = sent1_tokens + sent2_tokens\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_id = pad_sequences([input_id], maxlen=max_seq_len, value=tokenizer.pad_token_id, padding='post')[0]\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        input_ids.append(input_id)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e3fd675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 941814/941814 [02:01<00:00, 7776.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터에 대한 전처리\n",
    "X_train = convert_examples_to_features(train_data['sentence1'], train_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3642a",
   "metadata": {},
   "source": [
    "훈련 데이터의 첫번째 샘플에 대한 정수 인코딩, 정수 인코딩을 기존의 문자열로 복원한 결과는 다음과 같습니다. (데이터가 섞일 때는 랜덤이므로 저자와 첫번째 샘플은 다를 수 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "074a36a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어에 대한 정수 인코딩 : [    0  9253  9902 21725  9136 25625 21075  8662  8137 14087 23874  9407\n",
      " 20350  7301  8137  9676 10960     1     0  9018 21725  9177  9880 27198\n",
      "  8368 10546  9731  9016     1     9     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3]\n",
      "각 인코딩의 길이 : 128\n",
      "정수 인코딩 복원 : <s> 8명의 소녀들이 노란색 유니폼을 입고 밖에서 치어리딩을 하고 있다.</s><s> 이 소녀들은 각각 휠체어를 탄다.</s><unused0><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이: 128\n",
    "input_id = X_train[0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739b015",
   "metadata": {},
   "source": [
    "검증 데이터에 대해서도 전처리를 진행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a678d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2490/2490 [00:00<00:00, 7190.13it/s]\n"
     ]
    }
   ],
   "source": [
    "X_val = convert_examples_to_features(val_data['sentence1'], val_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc98cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어에 대한 정수 인코딩 : [    0  9394  9871  9135  8718 14364 10063  8013 37144  9265 12583  8006\n",
      " 25856   377     1     0  9258 10192  9848 11001 10644 10396 18796 20485\n",
      " 37472  9134 35673  9539 18174     1     9     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3     3     3     3     3\n",
      "     3     3     3     3     3     3     3     3]\n",
      "각 인코딩의 길이 : 128\n",
      "정수 인코딩 복원 : <s> 그리고 그가 말했다, \"엄마, 저 왔어요.\"</s><s> 그는 학교 버스가 그를 내려주자마자 엄마에게 전화를 걸었다.</s><unused0><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이: 128\n",
    "input_id = X_val[0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf03fa",
   "metadata": {},
   "source": [
    "테스트 데이터에 대해서 전처리를 진행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86123a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5010/5010 [00:00<00:00, 7357.80it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = convert_examples_to_features(test_data['sentence1'], test_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af611874",
   "metadata": {},
   "source": [
    "contradiction, entailment, neutral과 같이 문자열로 구성된 레이블에 대해서도 정수 인코딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01ba7a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 레이블과 정수 : {'contradiction': 0, 'entailment': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "train_label = train_data['gold_label'].tolist()\n",
    "val_label = val_data['gold_label'].tolist()\n",
    "test_label = test_data['gold_label'].tolist()\n",
    "\n",
    "idx_encode = preprocessing.LabelEncoder()\n",
    "# print(type(idx_encode))\n",
    "idx_encode.fit(train_label) # 검증, 테스트는 하지 않아도 됨. 여기에 다 포함되기 때문에\n",
    "\n",
    "# 고유한 정수로 변환. 고유한 정수라 해봤자 3개니까 0,1,2 이다.\n",
    "y_train = idx_encode.transform(train_label)\n",
    "y_val = idx_encode.transform(val_label) \n",
    "y_test = idx_encode.transform(test_label)\n",
    "\n",
    "\n",
    "label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\n",
    "idx_label = {value: key for key, value in label_idx.items()}\n",
    "print('각 레이블과 정수 :', label_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "943ab7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환 전 : ['contradiction', 'entailment', 'neutral', 'neutral', 'entailment']\n",
      "변환 후 : [0 1 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "print('변환 전 :', test_label[:5])\n",
    "print('변환 후 :',y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa9402",
   "metadata": {},
   "source": [
    "## GPT 출력 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3466455",
   "metadata": {},
   "source": [
    "koGPT-2를 이용해 모델을 구현하기 위해서는 koGPT-2의 출력을 이해할 필요가 있습니다. 우선, 한국어 GPT-2인 skt/kogpt2-base-v2를 로드해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d06e102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user01\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2Model: ['transformer.h.1.attn.masked_bias', 'lm_head.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2Model.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f1632e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_keras as keras\n",
    "\n",
    "max_seq_len = 128\n",
    "\n",
    "# input_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "input_ids_layer = keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "outputs = model([input_ids_layer])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31bae7",
   "metadata": {},
   "source": [
    "outputs에는 두 개의 출력이 존재하는데 인덱스 0을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82aca98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 128, 768), dtype=tf.float32, name=None), name='tfgpt2_model/transformer/Reshape_2:0', description=\"created by layer 'tfgpt2_model'\")\n"
     ]
    }
   ],
   "source": [
    "# 문장 길이만큼의 출력\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b98a7d",
   "metadata": {},
   "source": [
    "outputs[0]은 (배치 크기, 128, 768)의 크기를 가지는 텐서입니다. 이는 768차원의 벡터가 128개가 있다는 의미로 문장 길이 개수만큼의 출력을 얻었음을 의미합니다. 텍스트 분류 문제를 풀 경우에는 koGPT-2의 마지막 예측에 해당하는 벡터를 사용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bf68643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name='tf.__operators__.getitem/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem'\")\n"
     ]
    }
   ],
   "source": [
    "# 마지막 출력 벡터\n",
    "print(outputs[0][:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a78bdaa",
   "metadata": {},
   "source": [
    "## 여기서 부터가 진짜. 위에는 OUTPUT에 어떤 정보가 있는지 확인 차원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03263ddc",
   "metadata": {},
   "source": [
    "서브클래싱 구현 방식으로 구현한 텍스트 분류 모델은 다음과 같습니다. GPT의 출력 중 outputs[0][:, -1]. 즉, 마지막 출력 벡터를 소프트맥스 함수가 활성화 함수로 설정된 출력층으로 연결합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3d6e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFGPT2ForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(TFGPT2ForSequenceClassification, self).__init__()\n",
    "        self.gpt = TFGPT2Model.from_pretrained(model_name, from_pt=True)\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                activation='softmax',\n",
    "                                                name='classifier')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.gpt(input_ids=inputs)\n",
    "        cls_token = outputs[0][:, -1]\n",
    "        prediction = self.classifier(cls_token)\n",
    "\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f17ecb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2Model: ['transformer.h.1.attn.masked_bias', 'lm_head.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2ForSequenceClassification(\"skt/kogpt2-base-v2\", num_labels=3)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.001,\n",
    "    patience=2)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, epochs=2, batch_size=32, validation_data = (X_val, y_val),\n",
    "    callbacks = [early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ba149",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=1024)\n",
    "print(\"test loss, test acc: \", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
