{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 이 예제는 그냥 인코더만 있는 예제. 별 의미는 없는 듯",
   "id": "904c6aa9e0bd5ba8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.chdir(r\"c:\\projects\\PY_MLE\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, GlobalAveragePooling1D, SimpleRNN, Embedding, Masking, Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "import sentencepiece as spm\n",
    "import urllib.request\n",
    "import csv\n",
    "import unicodedata"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:22:00.397530Z",
     "start_time": "2025-12-11T15:21:57.833563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 10000\n",
    "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ],
   "id": "4c01099cfc530b34",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:22:01.125193600Z",
     "start_time": "2025-12-11T15:22:01.097758700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"최대 길이: {}\".format(max(len(x) for x in x_train)))\n",
    "print(\"평균 길이: {}\".format(sum(map(len, x_train))/len(x_train)))"
   ],
   "id": "512c7fe4a718e280",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이: 2494\n",
      "평균 길이: 238.71364\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:22:03.533958100Z",
     "start_time": "2025-12-11T15:22:02.993656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 패팅을 수행\n",
    "max_len = 500\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)"
   ],
   "id": "b4959d019782257e",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:32:38.409292900Z",
     "start_time": "2025-12-11T15:32:36.305963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "embedded_sequences = Embedding(vocab_size, 128, input_length=max_len, mask_zero=True)(sequence_input)\n",
    "lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences=True))(embedded_sequences)\n",
    "\n",
    "# return_state를 했기 때문에 forwar_h...에는 마지막값 저장되어 있을 뜻, lstm모든 값이 저장되어 있을 듯\n",
    "lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)"
   ],
   "id": "b2fed78ac62dc56b",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:33:01.360508200Z",
     "start_time": "2025-12-11T15:33:01.331994400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# state_h는 인코더의 마지막 은닉상택값이다. 왜 정방향의 마지막값과 역방향의 시작값을 연결한게 마지막시점의 은닉상태값인는 2장 211쪽에 잘 나와있다\n",
    "# 만약에 정방향 마지막 시점의 값과 동일 시점의 역바향값을 연결해서 그 시점의 은닉상태값으로 할 경우\n",
    "# 역방향은 모든 시점의 은닉상태값을 참조한게 아니기 때문이다. 결국 이런 결론에 이룰 수 있다. 정뱡향 어느 시점의 은닉상태값은\n",
    "# 그 시점의 정방향 값과 역방향 시작점 값을 연결하는 것이다. 역방향 시작점은 무조건 시점 1이 겠지\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n"
   ],
   "id": "f424a16d495ce88c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 500, 128) (None, 64) (None, 64) (None, 64) (None, 64)\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:33:14.881019900Z",
     "start_time": "2025-12-11T15:33:14.864522200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = Dense(units)\n",
    "    self.W2 = Dense(units)\n",
    "    self.V = Dense(1)\n",
    "\n",
    "  def call(self, values, query): # 단, key와 value는 같음\n",
    "    # query shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n"
   ],
   "id": "f89f89bc79ab2c6f",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b89a6f6095836200"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:34:23.072980600Z",
     "start_time": "2025-12-11T15:34:22.998006900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attention = BahdanauAttention(64)\n",
    "# lstm은 각 시점의 모든 은닉상태, state_h는 마지막 시점의 은닉상택값이다.\n",
    "\n",
    "context_vector, attention_weight = attention(lstm, state_h)\n",
    "dense1 = Dense(20, activation=\"relu\")(context_vector)\n",
    "dropout = Dropout(0.5)(dense1)\n",
    "output = Dense(1, activation=\"sigmoid\")(dropout)\n",
    "model = Model(inputs=sequence_input, outputs=output)"
   ],
   "id": "ccf9a6f3b4d40e98",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:35:20.484785400Z",
     "start_time": "2025-12-11T15:35:20.471533900Z"
    }
   },
   "cell_type": "code",
   "source": "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy' ])",
   "id": "803eb555a8433efe",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-11T15:35:31.430309400Z"
    }
   },
   "cell_type": "code",
   "source": "history = model.fit(x_train, y_train, epochs = 3, batch_size = 256, validation_data=(x_test, y_test), verbose=1)",
   "id": "b7b9b2271e907719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\user06\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user06\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "98/98 [==============================] - 1181s 12s/step - loss: 0.4732 - accuracy: 0.7655 - val_loss: 0.2842 - val_accuracy: 0.8806\n",
      "Epoch 2/3\n",
      "38/98 [==========>...................] - ETA: 10:43 - loss: 0.2306 - accuracy: 0.9197"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
