{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38e0e76",
   "metadata": {},
   "source": [
    "KorQuard가 2.0이 나왔는데 책보고 연습할려고 그냥 1.0 버전으로 수행 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ade7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizerFast\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "import sys\n",
    "sys.setrecursionlimit(10**7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2544caba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:02.346903100Z",
     "start_time": "2025-12-25T04:23:02.296492500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 파일을 로드하는 함수를 작성하자\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb0396fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:03.226459400Z",
     "start_time": "2025-12-25T04:23:02.349906400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_squad('KorQuAD_v1.0_train.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('KorQuAD_v1.0_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acbd7bb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:03.442699500Z",
     "start_time": "2025-12-25T04:23:03.341464800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련데이터 본문 개수:  60407\n",
      "훈련데이터 질문 개수:  60407\n",
      "훈련데이터 답변 개수:  60407\n",
      "테스트데이터 본문 개수:  5774\n",
      "테스트이터 질문 개수:  5774\n",
      "테스트데이터 답변 개수:  5774\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 탐색하자\n",
    "print(\"훈련데이터 본문 개수: \", len(train_contexts))\n",
    "print(\"훈련데이터 질문 개수: \", len(train_questions))\n",
    "print(\"훈련데이터 답변 개수: \", len(train_answers))\n",
    "\n",
    "print(\"테스트데이터 본문 개수: \", len(val_contexts))\n",
    "print(\"테스트이터 질문 개수: \", len(val_questions))\n",
    "print(\"테스트데이터 답변 개수: \", len(val_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8e7c188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:03.520564900Z",
     "start_time": "2025-12-25T04:23:03.452014600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "{'text': '교향곡', 'answer_start': 54}\n"
     ]
    }
   ],
   "source": [
    "print(train_contexts[0])\n",
    "print(train_questions[0])\n",
    "print(train_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b76ab60c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:03.589146Z",
     "start_time": "2025-12-25T04:23:03.525564Z"
    }
   },
   "outputs": [],
   "source": [
    "# 답변에는 시작인덱스만 있어서 종료인덱스를 만드는 함수를 구현한다.\n",
    "# 시작인덱스에 정답택스트 길이를 더하여 구한다.\n",
    "\n",
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        \n",
    "        # 뒤에 공백이 있으면 제거\n",
    "        answer['text'] = answer['text'].rstrip()\n",
    "        \n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        answer['answer_end'] = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adb3e940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:03.662781700Z",
     "start_time": "2025-12-25T04:23:03.591147400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 학습데이터 테스트데이터 모두 답변에 대한 종료 인덱스를 만든다.\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24b4e087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:03.733725Z",
     "start_time": "2025-12-25T04:23:03.667781700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '교향곡', 'answer_start': 54, 'answer_end': 57}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 답변에 대한 위치 확인.  학습데이터만 확인해 보자\n",
    "train_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b16d74fe55d4e47a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:18.615079500Z",
     "start_time": "2025-12-25T04:23:03.739087400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 기존 토큰나이저를 불러와서 이용한다.\n",
    "tokenizer = BertTokenizerFast.from_pretrained('klue/bert-base')\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9506313fef37ff8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:25.087958500Z",
     "start_time": "2025-12-25T04:23:25.011196600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '183',\n",
       " '##9',\n",
       " '##년',\n",
       " '바그너',\n",
       " '##는',\n",
       " '괴테',\n",
       " '##의',\n",
       " '파우',\n",
       " '##스트',\n",
       " '##을',\n",
       " '처음',\n",
       " '읽',\n",
       " '##고',\n",
       " '그',\n",
       " '내용',\n",
       " '##에',\n",
       " '마음',\n",
       " '##이',\n",
       " '끌려',\n",
       " '이를',\n",
       " '소재',\n",
       " '##로',\n",
       " '해서',\n",
       " '하나',\n",
       " '##의',\n",
       " '교향곡',\n",
       " '##을',\n",
       " '쓰',\n",
       " '##려',\n",
       " '##는',\n",
       " '뜻',\n",
       " '##을',\n",
       " '갖',\n",
       " '##는',\n",
       " '##다',\n",
       " '.',\n",
       " '이',\n",
       " '시기',\n",
       " '바그너',\n",
       " '##는',\n",
       " '183',\n",
       " '##8',\n",
       " '##년',\n",
       " '##에',\n",
       " '빛',\n",
       " '독촉',\n",
       " '##으로',\n",
       " '산전',\n",
       " '##수',\n",
       " '##전',\n",
       " '##을',\n",
       " '다',\n",
       " '[UNK]',\n",
       " '상황',\n",
       " '##이',\n",
       " '##라',\n",
       " '좌절',\n",
       " '##과',\n",
       " '실망',\n",
       " '##에',\n",
       " '가득',\n",
       " '##했',\n",
       " '##으며',\n",
       " '메',\n",
       " '##피스',\n",
       " '##토',\n",
       " '##펠',\n",
       " '##레스',\n",
       " '##를',\n",
       " '만나',\n",
       " '##는',\n",
       " '파우',\n",
       " '##스트',\n",
       " '##의',\n",
       " '심경',\n",
       " '##에',\n",
       " '공감',\n",
       " '##했',\n",
       " '##다고',\n",
       " '한다',\n",
       " '.',\n",
       " '또한',\n",
       " '파리',\n",
       " '##에서',\n",
       " '아',\n",
       " '##브',\n",
       " '##네',\n",
       " '##크',\n",
       " '##의',\n",
       " '지휘',\n",
       " '##로',\n",
       " '파리',\n",
       " '음악',\n",
       " '##원',\n",
       " '관현',\n",
       " '##악단',\n",
       " '##이',\n",
       " '연주',\n",
       " '##하',\n",
       " '##는',\n",
       " '베토벤',\n",
       " '##의',\n",
       " '교향곡',\n",
       " '9',\n",
       " '##번',\n",
       " '##을',\n",
       " '듣',\n",
       " '##고',\n",
       " '깊',\n",
       " '##은',\n",
       " '감명',\n",
       " '##을',\n",
       " '받',\n",
       " '##았',\n",
       " '##는데',\n",
       " ',',\n",
       " '이것',\n",
       " '##이',\n",
       " '이듬해',\n",
       " '1',\n",
       " '##월',\n",
       " '##에',\n",
       " '파우',\n",
       " '##스트',\n",
       " '##의',\n",
       " '서',\n",
       " '##곡',\n",
       " '##으로',\n",
       " '쓰여진',\n",
       " '이',\n",
       " '작품',\n",
       " '##에',\n",
       " '조금',\n",
       " '##이',\n",
       " '##라도',\n",
       " '영향',\n",
       " '##을',\n",
       " '끼쳤',\n",
       " '##으리',\n",
       " '##라는',\n",
       " '것',\n",
       " '##은',\n",
       " '의심',\n",
       " '##할',\n",
       " '여지',\n",
       " '##가',\n",
       " '없',\n",
       " '##다',\n",
       " '.',\n",
       " '여기',\n",
       " '##의',\n",
       " '라',\n",
       " '##단',\n",
       " '##조',\n",
       " '조성',\n",
       " '##의',\n",
       " '경우',\n",
       " '##에도',\n",
       " '그',\n",
       " '##의',\n",
       " '전기',\n",
       " '##에',\n",
       " '적혀',\n",
       " '있',\n",
       " '##는',\n",
       " '것',\n",
       " '##처럼',\n",
       " '단순',\n",
       " '##한',\n",
       " '정신',\n",
       " '##적',\n",
       " '피로',\n",
       " '##나',\n",
       " '실',\n",
       " '##의',\n",
       " '##가',\n",
       " '반영',\n",
       " '##된',\n",
       " '것',\n",
       " '##이',\n",
       " '아니',\n",
       " '##라',\n",
       " '베토벤',\n",
       " '##의',\n",
       " '합창',\n",
       " '##교',\n",
       " '##향',\n",
       " '##곡',\n",
       " '조성',\n",
       " '##의',\n",
       " '영향',\n",
       " '##을',\n",
       " '받',\n",
       " '##은',\n",
       " '것',\n",
       " '##을',\n",
       " '볼',\n",
       " '수',\n",
       " '있',\n",
       " '##다',\n",
       " '.',\n",
       " '그렇게',\n",
       " '교향곡',\n",
       " '작곡',\n",
       " '##을',\n",
       " '183',\n",
       " '##9',\n",
       " '##년',\n",
       " '##부터',\n",
       " '40',\n",
       " '##년',\n",
       " '##에',\n",
       " '걸쳐',\n",
       " '파리',\n",
       " '##에서',\n",
       " '착수',\n",
       " '##했',\n",
       " '##으나',\n",
       " '1',\n",
       " '##악',\n",
       " '##장',\n",
       " '##을',\n",
       " '쓴',\n",
       " '뒤',\n",
       " '##에',\n",
       " '중단',\n",
       " '##했',\n",
       " '##다',\n",
       " '.',\n",
       " '또한',\n",
       " '작품',\n",
       " '##의',\n",
       " '완성',\n",
       " '##과',\n",
       " '동시',\n",
       " '##에',\n",
       " '그',\n",
       " '##는',\n",
       " '이',\n",
       " '서',\n",
       " '##곡',\n",
       " '(',\n",
       " '1',\n",
       " '##악',\n",
       " '##장',\n",
       " ')',\n",
       " '을',\n",
       " '파리',\n",
       " '음악',\n",
       " '##원',\n",
       " '##의',\n",
       " '연주회',\n",
       " '##에서',\n",
       " '연주',\n",
       " '##할',\n",
       " '파트',\n",
       " '##보',\n",
       " '##까',\n",
       " '##지',\n",
       " '준비',\n",
       " '##하',\n",
       " '##였',\n",
       " '##으나',\n",
       " ',',\n",
       " '실제로',\n",
       " '##는',\n",
       " '이루어지',\n",
       " '##지',\n",
       " '##는',\n",
       " '않',\n",
       " '##았',\n",
       " '##다',\n",
       " '.',\n",
       " '결국',\n",
       " '초연',\n",
       " '##은',\n",
       " '4',\n",
       " '##년',\n",
       " '반',\n",
       " '##이',\n",
       " '지난',\n",
       " '후',\n",
       " '##에',\n",
       " '드레스',\n",
       " '##덴',\n",
       " '##에서',\n",
       " '연주',\n",
       " '##되',\n",
       " '##었',\n",
       " '##고',\n",
       " '재연',\n",
       " '##도',\n",
       " '이루',\n",
       " '##어졌',\n",
       " '##지만',\n",
       " ',',\n",
       " '이후',\n",
       " '##에',\n",
       " '그대로',\n",
       " '방치',\n",
       " '##되',\n",
       " '##고',\n",
       " '말',\n",
       " '##았',\n",
       " '##다',\n",
       " '.',\n",
       " '그',\n",
       " '사이',\n",
       " '##에',\n",
       " '그',\n",
       " '##는',\n",
       " '리',\n",
       " '##엔',\n",
       " '##치',\n",
       " '##와',\n",
       " '방황',\n",
       " '##하',\n",
       " '##는',\n",
       " '네덜란드',\n",
       " '##인',\n",
       " '##을',\n",
       " '완성',\n",
       " '##하고',\n",
       " '탄',\n",
       " '##호',\n",
       " '##이',\n",
       " '##저',\n",
       " '##에도',\n",
       " '착수',\n",
       " '##하',\n",
       " '##는',\n",
       " '등',\n",
       " '분주',\n",
       " '##한',\n",
       " '시간',\n",
       " '##을',\n",
       " '보냈',\n",
       " '##는데',\n",
       " ',',\n",
       " '그런',\n",
       " '바쁜',\n",
       " '생활',\n",
       " '##이',\n",
       " '이',\n",
       " '곡',\n",
       " '##을',\n",
       " '잊',\n",
       " '##게',\n",
       " '한',\n",
       " '것',\n",
       " '##이',\n",
       " '아닌가',\n",
       " '하',\n",
       " '##는',\n",
       " '의견',\n",
       " '##도',\n",
       " '있',\n",
       " '##다',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '바그너',\n",
       " '##는',\n",
       " '괴테',\n",
       " '##의',\n",
       " '파우',\n",
       " '##스트',\n",
       " '##를',\n",
       " '읽',\n",
       " '##고',\n",
       " '무엇',\n",
       " '##을',\n",
       " '쓰',\n",
       " '##고',\n",
       " '##자',\n",
       " '했',\n",
       " '##는',\n",
       " '##가',\n",
       " '?',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a665b02da528912d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:25.149074800Z",
     "start_time": "2025-12-25T04:23:25.090959800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 13934,\n",
       " 2236,\n",
       " 2440,\n",
       " 27982,\n",
       " 2259,\n",
       " 21310,\n",
       " 2079,\n",
       " 11994,\n",
       " 3791,\n",
       " 2069,\n",
       " 3790,\n",
       " 1508,\n",
       " 2088,\n",
       " 636,\n",
       " 3800,\n",
       " 2170,\n",
       " 3717,\n",
       " 2052,\n",
       " 9001,\n",
       " 8345,\n",
       " 4642,\n",
       " 2200,\n",
       " 3689,\n",
       " 3657,\n",
       " 2079,\n",
       " 19282,\n",
       " 2069,\n",
       " 1363,\n",
       " 2370,\n",
       " 2259,\n",
       " 936,\n",
       " 2069,\n",
       " 554,\n",
       " 2259,\n",
       " 2062,\n",
       " 18,\n",
       " 1504,\n",
       " 4342,\n",
       " 27982,\n",
       " 2259,\n",
       " 13934,\n",
       " 2196,\n",
       " 2440,\n",
       " 2170,\n",
       " 1195,\n",
       " 23260,\n",
       " 6233,\n",
       " 17370,\n",
       " 2113,\n",
       " 2165,\n",
       " 2069,\n",
       " 809,\n",
       " 1,\n",
       " 3706,\n",
       " 2052,\n",
       " 2181,\n",
       " 8642,\n",
       " 2145,\n",
       " 7334,\n",
       " 2170,\n",
       " 4983,\n",
       " 2371,\n",
       " 4007,\n",
       " 1065,\n",
       " 5917,\n",
       " 2386,\n",
       " 2559,\n",
       " 4443,\n",
       " 2138,\n",
       " 4026,\n",
       " 2259,\n",
       " 11994,\n",
       " 3791,\n",
       " 2079,\n",
       " 15864,\n",
       " 2170,\n",
       " 5487,\n",
       " 2371,\n",
       " 4683,\n",
       " 3605,\n",
       " 18,\n",
       " 3819,\n",
       " 5986,\n",
       " 27135,\n",
       " 1376,\n",
       " 2645,\n",
       " 2203,\n",
       " 2292,\n",
       " 2079,\n",
       " 5872,\n",
       " 2200,\n",
       " 5986,\n",
       " 4152,\n",
       " 2252,\n",
       " 22835,\n",
       " 16706,\n",
       " 2052,\n",
       " 5485,\n",
       " 2205,\n",
       " 2259,\n",
       " 17087,\n",
       " 2079,\n",
       " 19282,\n",
       " 29,\n",
       " 2517,\n",
       " 2069,\n",
       " 881,\n",
       " 2088,\n",
       " 652,\n",
       " 2073,\n",
       " 23404,\n",
       " 2069,\n",
       " 1122,\n",
       " 2886,\n",
       " 13964,\n",
       " 16,\n",
       " 3982,\n",
       " 2052,\n",
       " 9944,\n",
       " 21,\n",
       " 2429,\n",
       " 2170,\n",
       " 11994,\n",
       " 3791,\n",
       " 2079,\n",
       " 1258,\n",
       " 2465,\n",
       " 6233,\n",
       " 24294,\n",
       " 1504,\n",
       " 3967,\n",
       " 2170,\n",
       " 4027,\n",
       " 2052,\n",
       " 5121,\n",
       " 3979,\n",
       " 2069,\n",
       " 18274,\n",
       " 21575,\n",
       " 23548,\n",
       " 575,\n",
       " 2073,\n",
       " 5292,\n",
       " 2085,\n",
       " 7251,\n",
       " 2116,\n",
       " 1415,\n",
       " 2062,\n",
       " 18,\n",
       " 3776,\n",
       " 2079,\n",
       " 942,\n",
       " 2286,\n",
       " 2446,\n",
       " 4196,\n",
       " 2079,\n",
       " 3640,\n",
       " 6509,\n",
       " 636,\n",
       " 2079,\n",
       " 4450,\n",
       " 2170,\n",
       " 10329,\n",
       " 1513,\n",
       " 2259,\n",
       " 575,\n",
       " 7925,\n",
       " 4488,\n",
       " 2470,\n",
       " 4006,\n",
       " 2125,\n",
       " 7874,\n",
       " 2075,\n",
       " 1329,\n",
       " 2079,\n",
       " 2116,\n",
       " 4523,\n",
       " 2897,\n",
       " 575,\n",
       " 2052,\n",
       " 3614,\n",
       " 2181,\n",
       " 17087,\n",
       " 2079,\n",
       " 10044,\n",
       " 2120,\n",
       " 2482,\n",
       " 2465,\n",
       " 4196,\n",
       " 2079,\n",
       " 3979,\n",
       " 2069,\n",
       " 1122,\n",
       " 2073,\n",
       " 575,\n",
       " 2069,\n",
       " 1164,\n",
       " 1295,\n",
       " 1513,\n",
       " 2062,\n",
       " 18,\n",
       " 3914,\n",
       " 19282,\n",
       " 8067,\n",
       " 2069,\n",
       " 13934,\n",
       " 2236,\n",
       " 2440,\n",
       " 3797,\n",
       " 4064,\n",
       " 2440,\n",
       " 2170,\n",
       " 5314,\n",
       " 5986,\n",
       " 27135,\n",
       " 7615,\n",
       " 2371,\n",
       " 4381,\n",
       " 21,\n",
       " 2376,\n",
       " 2121,\n",
       " 2069,\n",
       " 1365,\n",
       " 873,\n",
       " 2170,\n",
       " 4967,\n",
       " 2371,\n",
       " 2062,\n",
       " 18,\n",
       " 3819,\n",
       " 3967,\n",
       " 2079,\n",
       " 4976,\n",
       " 2145,\n",
       " 4213,\n",
       " 2170,\n",
       " 636,\n",
       " 2259,\n",
       " 1504,\n",
       " 1258,\n",
       " 2465,\n",
       " 12,\n",
       " 21,\n",
       " 2376,\n",
       " 2121,\n",
       " 13,\n",
       " 1498,\n",
       " 5986,\n",
       " 4152,\n",
       " 2252,\n",
       " 2079,\n",
       " 16385,\n",
       " 27135,\n",
       " 5485,\n",
       " 2085,\n",
       " 6237,\n",
       " 2178,\n",
       " 2299,\n",
       " 2118,\n",
       " 3864,\n",
       " 2205,\n",
       " 2507,\n",
       " 4381,\n",
       " 16,\n",
       " 4539,\n",
       " 2259,\n",
       " 6268,\n",
       " 2118,\n",
       " 2259,\n",
       " 1380,\n",
       " 2886,\n",
       " 2062,\n",
       " 18,\n",
       " 3983,\n",
       " 16087,\n",
       " 2073,\n",
       " 24,\n",
       " 2440,\n",
       " 1121,\n",
       " 2052,\n",
       " 3625,\n",
       " 1943,\n",
       " 2170,\n",
       " 9605,\n",
       " 2906,\n",
       " 27135,\n",
       " 5485,\n",
       " 2496,\n",
       " 2359,\n",
       " 2088,\n",
       " 17807,\n",
       " 2119,\n",
       " 4046,\n",
       " 6028,\n",
       " 3683,\n",
       " 16,\n",
       " 3719,\n",
       " 2170,\n",
       " 4311,\n",
       " 8095,\n",
       " 2496,\n",
       " 2088,\n",
       " 1041,\n",
       " 2886,\n",
       " 2062,\n",
       " 18,\n",
       " 636,\n",
       " 3734,\n",
       " 2170,\n",
       " 636,\n",
       " 2259,\n",
       " 1028,\n",
       " 2614,\n",
       " 2225,\n",
       " 2522,\n",
       " 14231,\n",
       " 2205,\n",
       " 2259,\n",
       " 8445,\n",
       " 2179,\n",
       " 2069,\n",
       " 4976,\n",
       " 19521,\n",
       " 1763,\n",
       " 2016,\n",
       " 2052,\n",
       " 2190,\n",
       " 6509,\n",
       " 7615,\n",
       " 2205,\n",
       " 2259,\n",
       " 886,\n",
       " 12100,\n",
       " 2470,\n",
       " 3641,\n",
       " 2069,\n",
       " 5755,\n",
       " 13964,\n",
       " 16,\n",
       " 3637,\n",
       " 9064,\n",
       " 3799,\n",
       " 2052,\n",
       " 1504,\n",
       " 595,\n",
       " 2069,\n",
       " 1515,\n",
       " 2318,\n",
       " 1891,\n",
       " 575,\n",
       " 2052,\n",
       " 5215,\n",
       " 1889,\n",
       " 2259,\n",
       " 4192,\n",
       " 2119,\n",
       " 1513,\n",
       " 2062,\n",
       " 18,\n",
       " 3,\n",
       " 27982,\n",
       " 2259,\n",
       " 21310,\n",
       " 2079,\n",
       " 11994,\n",
       " 3791,\n",
       " 2138,\n",
       " 1508,\n",
       " 2088,\n",
       " 3890,\n",
       " 2069,\n",
       " 1363,\n",
       " 2088,\n",
       " 2155,\n",
       " 1902,\n",
       " 2259,\n",
       " 2116,\n",
       " 35,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0].ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44d0569192faf413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:25.190993900Z",
     "start_time": "2025-12-25T04:23:25.152074400Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    deleting_list = []\n",
    "\n",
    "    for i in tqdm(range(len(answers))):\n",
    "        # 토큰화전 문자의 인덱스(start, end)로부터 토큰화한 후의 문자의 인덱스를 찾아냄\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] -1))\n",
    "\n",
    "        # 시작 인덱스가 본문에 없는 경우. 즉 문장이 512보다 길어서 짤린 경우\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "            deleting_list.append(i)\n",
    "\n",
    "        # 종료인덱스가 없는 경우\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "            if i not in deleting_list:\n",
    "                deleting_list.append(i)\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return deleting_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d1adb3fce179769",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:25.424036900Z",
     "start_time": "2025-12-25T04:23:25.197004200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60407 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60407/60407 [00:00<00:00, 425937.18it/s]\n",
      "100%|██████████| 5774/5774 [00:00<00:00, 576356.20it/s]\n"
     ]
    }
   ],
   "source": [
    "deleting_list_for_train = add_token_positions(train_encodings, train_answers)\n",
    "deleting_list_for_test = add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aafd2df3a9026bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:23:32.519850500Z",
     "start_time": "2025-12-25T04:23:32.422955200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc81ac588369bf22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T04:24:07.524471500Z",
     "start_time": "2025-12-25T04:24:07.484916300Z"
    }
   },
   "outputs": [],
   "source": [
    "# deleting_list_for train에 있는 항목은 학습에서 제거해야 함\n",
    "def delete_samples(encodings, deleting_list):\n",
    "    input_ids = np.delete(np.array(encodings['input_ids']), deleting_list, axis =0)\n",
    "    attention_masks = np.delete(np.array(encodings['attention_mask']),deleting_list, axis=0)\n",
    "    start_positions = np.delete(np.array(encodings['start_positions']), deleting_list, axis=0)\n",
    "    end_positions = np.delete(np.array(encodings['end_positions']), deleting_list , axis=0)\n",
    "\n",
    "    X_data = [input_ids, attention_masks]\n",
    "    y_data = [start_positions, end_positions]\n",
    "\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "977ce2736a852794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T05:50:14.157679Z",
     "start_time": "2025-12-25T05:50:11.827643400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 답변이 없는 값을 제거한 리스트 형태이다\n",
    "X_train, y_train = delete_samples(train_encodings, deleting_list_for_train)\n",
    "X_test, y_test = delete_samples(val_encodings, deleting_list_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "734d76da182037ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:17:10.469749300Z",
     "start_time": "2025-12-25T06:17:10.453013700Z"
    }
   },
   "outputs": [],
   "source": [
    "class TFBertForQuestionAnswering(tf.keras.Model):\n",
    "    def __init__(self, model_name):\n",
    "        super(TFBertForQuestionAnswering, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
    "        self.qa_outputs = tf.keras.layers.Dense(2,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                name='qa_outputs')\n",
    "        self.softmax = tf.keras.layers.Activation(tf.keras.activations.softmax)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "        # bert는 여러층으로 되어 있는데 0번에 저장되나 보네\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "\n",
    "        # start_logits = (batch_size, sequence_length,)\n",
    "        # end_logits = (batch_size, sequence_length,)\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "\n",
    "        start_probs = self.softmax(start_logits)\n",
    "        end_probs = self.softmax(end_logits)\n",
    "\n",
    "        return start_probs, end_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ca3dd9d98c878ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:16:30.829679500Z",
     "start_time": "2025-12-25T06:16:28.531559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'bert.embeddings.position_ids', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForQuestionAnswering(\"klue/bert-base\")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "model.compile(optimizer=optimizer, loss=[loss, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6c73169810eb6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-25T06:19:36.998069900Z"
    }
   },
   "outputs": [],
   "source": [
    "# 시간이 많이 걸려서 왠만하며 패스\n",
    "history = model.fit(X_train,y_train, epochs=3, verbose=1, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d2b60",
   "metadata": {},
   "source": [
    "데이터를 예측하는 함수를 만들어 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5c5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의 ( 폭력행위등처벌에관한법률위반 ) 으로 지명수배되었다. 1989년 3월 12일 서울지방검찰청 공안부는 임종석의 사전구속영장을 발부받았다. 같은 해 6월 30일 평양축전에 임수경을 대표로 파견하여 국가보안법위반 혐의가 추가되었다. 경찰은 12월 18일 ~ 20일 사이 서울 경희대학교에서 임종석이 성명 발표를 추진하고 있다는 첩보를 입수했고, 12월 18일 오전 7시 40분 경 가스총과 전자봉으로 무장한 특공조 및 대공과 직원 12명 등 22명의 사복 경찰을 승용차 8대에 나누어 경희대학교에 투입했다. 1989년 12월 18일 오전 8시 15분 경 서울청량리경찰서는 호위 학생 5명과 함께 경희대학교 학생회관 건물 계단을 내려오는 임종석을 발견, 검거해 구속을 집행했다. 임종석은 청량리경찰서에서 약 1시간 동안 조사를 받은 뒤 오전 9시 50분 경 서울 장안동의 서울지방경찰청 공안분실로 인계되었다. ',\n",
       " '임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은? ',\n",
       " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_test_data_by_idx(idx):\n",
    "    context = tokenizer.decode(X_test[0][idx]).split('[SEP] ')[0]  # 본문\n",
    "    question = tokenizer.decode(X_test[0][idx]).split('[SEP] ')[1] # 질문\n",
    "    print('본문  :', context)\n",
    "    print('질문  :', question)\n",
    "    \n",
    "    # 답변인데 y_test에는 2개의 층이 있는데 1층(어레이)은 정답의 시작점, 다른 한층은 정답의 끝점이다.\n",
    "    # 그래서 아래의 코드가 성립이 되는 것이다.\n",
    "    answer_encoded = X_test[0][idx][y_test[0][idx]:y_test[1][idx]+1]\n",
    "    print('정답  :',tokenizer.decode(answer_encoded))\n",
    "    \n",
    "    # 위에는 기존에 있는 정답이고 아래부터는 예측을 통한 결과값을 의미한다.\n",
    "    output = model([tf.constant(X_test[0][idx])[None, :], tf.constant(X_test[1][ idx])[None, :]])\n",
    "    \n",
    "    # 정답의 시작인덱스와 종료 인텍스를 각각 구한다.\n",
    "    start = tf.math.argmax(tf.squeeze(output[0]))\n",
    "    end = tf.math.argmax(tf.squeeze(output[1]))+1\n",
    "    \n",
    "    # 예측해서 디코딩해서 출력한다.\n",
    "    answer_encoded = X_test[0][idx][start:end]\n",
    "    print('예측  :',tokenizer.decode(answer_encoded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
