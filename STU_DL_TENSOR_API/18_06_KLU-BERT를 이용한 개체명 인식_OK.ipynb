{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0875e3fe",
   "metadata": {},
   "source": [
    "KLUE-BERT 이것은 KLUE팀에서 만든 한국어 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa73dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user06\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import shape_list, BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "os.chdir(r\"c:\\projects\\PY_MLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f985fc",
   "metadata": {},
   "source": [
    "데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312636ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Tag",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7a41d2fe-776b-48d1-ac61-544981662786",
       "rows": [
        [
         "0",
         "정은 씨를 힘들게 한 가스나그, 가만둘 수 없겠죠 .",
         "PER-B O O O O O O O O"
        ],
        [
         "1",
         "▶ 쿠마리 한동수가 말하는 '가넷 & 에르덴'",
         "O PER-B PER-I O PER-B O PER-B"
        ],
        [
         "2",
         "슈나이더의 프레젠테이션은 말 청중을 위한 특별한 쇼다 .",
         "PER-B O O CVL-B O O O O"
        ],
        [
         "3",
         "지구 최대 연료탱크 수검 회사 구글이 연내 22명 안팎의 인력을 갖춘 연구개발(R&D)센터를 경궐 리버티섬 테헤란로에 세우고 코리아 시장에 본격 진출한다 .",
         "O O TRM-B O O ORG-B DAT-B NUM-B O O O ORG-B LOC-B LOC-B AFW-B O LOC-B O O O O"
        ],
        [
         "4",
         "5. <10:00:TI_HOUR> 도이치증권대 <0:1:QT_SPORTS> 연예오락심의위원회(종료)",
         "NUM-B O ORG-B O ORG-B"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>정은 씨를 힘들게 한 가스나그, 가만둘 수 없겠죠 .</td>\n",
       "      <td>PER-B O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▶ 쿠마리 한동수가 말하는 '가넷 &amp; 에르덴'</td>\n",
       "      <td>O PER-B PER-I O PER-B O PER-B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>슈나이더의 프레젠테이션은 말 청중을 위한 특별한 쇼다 .</td>\n",
       "      <td>PER-B O O CVL-B O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>지구 최대 연료탱크 수검 회사 구글이 연내 22명 안팎의 인력을 갖춘 연구개발(R&amp;...</td>\n",
       "      <td>O O TRM-B O O ORG-B DAT-B NUM-B O O O ORG-B LO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. &lt;10:00:TI_HOUR&gt; 도이치증권대 &lt;0:1:QT_SPORTS&gt; 연예오락...</td>\n",
       "      <td>NUM-B O ORG-B O ORG-B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0                      정은 씨를 힘들게 한 가스나그, 가만둘 수 없겠죠 .   \n",
       "1                          ▶ 쿠마리 한동수가 말하는 '가넷 & 에르덴'   \n",
       "2                    슈나이더의 프레젠테이션은 말 청중을 위한 특별한 쇼다 .   \n",
       "3  지구 최대 연료탱크 수검 회사 구글이 연내 22명 안팎의 인력을 갖춘 연구개발(R&...   \n",
       "4  5. <10:00:TI_HOUR> 도이치증권대 <0:1:QT_SPORTS> 연예오락...   \n",
       "\n",
       "                                                 Tag  \n",
       "0                              PER-B O O O O O O O O  \n",
       "1                      O PER-B PER-I O PER-B O PER-B  \n",
       "2                            PER-B O O CVL-B O O O O  \n",
       "3  O O TRM-B O O ORG-B DAT-B NUM-B O O O ORG-B LO...  \n",
       "4                              NUM-B O ORG-B O ORG-B  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner_df = pd.read_csv(\"ner_train_data.csv\")\n",
    "train_ner_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f28485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Tag",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f095c769-2209-4ada-9aea-5dd015a2d95b",
       "rows": [
        [
         "0",
         "라티은-원윤정, 휘닉스파크클래식 프로골퍼",
         "PER-B EVT-B CVL-B"
        ],
        [
         "1",
         "5원으로 맺어진 애인까지 돈이라는 민감한 원자재를 통해 현대인의 물질만능주의를 꼬집고 있는 이 무비는 .",
         "NUM-B O O O O O O O O O O O FLD-B O"
        ],
        [
         "2",
         "-날로 삼키면 맛이 어떤지 일차 드셔보시겠어요 .",
         "O O O O NUM-B O O"
        ],
        [
         "3",
         "-네, 지었습니다 .",
         "O O O"
        ],
        [
         "4",
         "◇신규 투자촉진에 방점=이번 접속료 조정결과에서 눈에 띄는 지점은 WCDMA/HSDPA 등 3세대(G) 초고속인터넷 뉴스핌 의 아치형(BcN) 출자분을 적극 반영했다는 점이다 .",
         "O O O O O O O O TRM-B O TRM-B TRM-I ORG-B O TRM-B O O O O O"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>라티은-원윤정, 휘닉스파크클래식 프로골퍼</td>\n",
       "      <td>PER-B EVT-B CVL-B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5원으로 맺어진 애인까지 돈이라는 민감한 원자재를 통해 현대인의 물질만능주의를 꼬집...</td>\n",
       "      <td>NUM-B O O O O O O O O O O O FLD-B O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-날로 삼키면 맛이 어떤지 일차 드셔보시겠어요 .</td>\n",
       "      <td>O O O O NUM-B O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-네, 지었습니다 .</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>◇신규 투자촉진에 방점=이번 접속료 조정결과에서 눈에 띄는 지점은 WCDMA/HSD...</td>\n",
       "      <td>O O O O O O O O TRM-B O TRM-B TRM-I ORG-B O TR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0                             라티은-원윤정, 휘닉스파크클래식 프로골퍼   \n",
       "1  5원으로 맺어진 애인까지 돈이라는 민감한 원자재를 통해 현대인의 물질만능주의를 꼬집...   \n",
       "2                        -날로 삼키면 맛이 어떤지 일차 드셔보시겠어요 .   \n",
       "3                                        -네, 지었습니다 .   \n",
       "4  ◇신규 투자촉진에 방점=이번 접속료 조정결과에서 눈에 띄는 지점은 WCDMA/HSD...   \n",
       "\n",
       "                                                 Tag  \n",
       "0                                  PER-B EVT-B CVL-B  \n",
       "1                NUM-B O O O O O O O O O O O FLD-B O  \n",
       "2                                  O O O O NUM-B O O  \n",
       "3                                              O O O  \n",
       "4  O O O O O O O O TRM-B O TRM-B TRM-I ORG-B O TR...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ner_df = pd.read_csv(\"ner_test_data.csv\")\n",
    "test_ner_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4ae1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ner_df))\n",
    "print(len(test_ner_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01f1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장과 레이블을 분리해서 저장. 문장도 하나의 리스트안에 토큰화 함\n",
    "train_data_sentence = [sent.split() for sent in train_ner_df['Sentence'].values]\n",
    "test_data_sentence = [sent.split() for sent in test_ner_df['Sentence'].values]\n",
    "train_data_label = [sent.split() for sent in train_ner_df['Tag'].values]\n",
    "test_data_label = [sent.split() for sent in test_ner_df['Tag'].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a7d648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['슈나이더의', '프레젠테이션은', '말', '청중을', '위한', '특별한', '쇼다', '.']\n",
      "['PER-B', 'O', 'O', 'CVL-B', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# 예를 들어서 2번 인덱스것을 찍어본다.\n",
    "print(train_data_sentence[2])\n",
    "print(train_data_label[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586ad0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'PER-B', 'PER-I', 'FLD-B', 'FLD-I', 'AFW-B', 'AFW-I', 'ORG-B', 'ORG-I', 'LOC-B', 'LOC-I', 'CVL-B', 'CVL-I', 'DAT-B', 'DAT-I', 'TIM-B', 'TIM-I', 'NUM-B', 'NUM-I', 'EVT-B', 'EVT-I', 'ANM-B', 'ANM-I', 'PLT-B', 'PLT-I', 'MAT-B', 'MAT-I', 'TRM-B', 'TRM-I']\n"
     ]
    }
   ],
   "source": [
    "# 개체명을 정의하고 있는 텍스트 파일을 읽어서 확인\n",
    "labels = [label.strip() for label in open(\"ner_label.txt\", 'r', encoding='utf-8')]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41b7f0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'PER-B': 1, 'PER-I': 2, 'FLD-B': 3, 'FLD-I': 4, 'AFW-B': 5, 'AFW-I': 6, 'ORG-B': 7, 'ORG-I': 8, 'LOC-B': 9, 'LOC-I': 10, 'CVL-B': 11, 'CVL-I': 12, 'DAT-B': 13, 'DAT-I': 14, 'TIM-B': 15, 'TIM-I': 16, 'NUM-B': 17, 'NUM-I': 18, 'EVT-B': 19, 'EVT-I': 20, 'ANM-B': 21, 'ANM-I': 22, 'PLT-B': 23, 'PLT-I': 24, 'MAT-B': 25, 'MAT-I': 26, 'TRM-B': 27, 'TRM-I': 28}\n",
      "{0: 'O', 1: 'PER-B', 2: 'PER-I', 3: 'FLD-B', 4: 'FLD-I', 5: 'AFW-B', 6: 'AFW-I', 7: 'ORG-B', 8: 'ORG-I', 9: 'LOC-B', 10: 'LOC-I', 11: 'CVL-B', 12: 'CVL-I', 13: 'DAT-B', 14: 'DAT-I', 15: 'TIM-B', 16: 'TIM-I', 17: 'NUM-B', 18: 'NUM-I', 19: 'EVT-B', 20: 'EVT-I', 21: 'ANM-B', 22: 'ANM-I', 23: 'PLT-B', 24: 'PLT-I', 25: 'MAT-B', 26: 'MAT-I', 27: 'TRM-B', 28: 'TRM-I'}\n"
     ]
    }
   ],
   "source": [
    "# 개체명 태깅정보와 정수로 매핑\n",
    "tag_to_index = {tag: index for index, tag in enumerate(labels)}\n",
    "index_to_tag = {index: tag for index, tag in enumerate(labels)}\n",
    "print(tag_to_index)\n",
    "print(index_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f23bb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "tag_size = len(tag_to_index)\n",
    "print(tag_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f338289",
   "metadata": {},
   "source": [
    "전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfe664f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50322aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 후 문장: ['▶', '쿠', '##마리', '한동', '##수', '##가', '말', '##하', '##는', \"'\", '가', '##넷', '&', '에르', '##덴', \"'\"]\n",
      "레이블: ['O', 'PER-B', '[PAD]', 'PER-I', '[PAD]', '[PAD]', 'O', '[PAD]', '[PAD]', 'PER-B', '[PAD]', '[PAD]', 'O', 'PER-B', '[PAD]', '[PAD]']\n",
      "레이블의 정수 인코딩: [0, 1, -100, 2, -100, -100, 0, -100, -100, 1, -100, -100, 0, 1, -100, -100]\n",
      "문장의 길이: 16\n",
      "레이블의 길이: 16\n"
     ]
    }
   ],
   "source": [
    "# 전처리 하기 전 하나의 데이터를 가지고 샘플로 하나 만들어 보자\n",
    "tokens = []\n",
    "labels_ids = []\n",
    "\n",
    "\n",
    "# train_data_sentence, train_data_label 둘다 리스트이다. zip으로 묶었기 때문에 요소끼리 하나의 쌍이 된다.\n",
    "for one_word, label_token in zip(train_data_sentence[1], train_data_label[1]):\n",
    "    subword_tokens = tokenizer.tokenize(one_word)\n",
    "    tokens.extend(subword_tokens)\n",
    "    \n",
    "    # todo  이문장이 놀랍다.\n",
    "    labels_ids.extend([tag_to_index[label_token]] + [-100] * (len(subword_tokens)-1)) # [0, -100] 이런식이다.\n",
    "    \n",
    "print('토큰화 후 문장:', tokens)\n",
    "print('레이블:', ['[PAD]' if idx == -100 else index_to_tag[idx] for idx in labels_ids])\n",
    "print('레이블의 정수 인코딩:', labels_ids)\n",
    "print('문장의 길이:', len(tokens)) # 이걸 문장의 길이라고 할 수 있나. ## 이 있는데\n",
    "print('레이블의 길이:',len(labels_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa6c0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리를 위한 함수\n",
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer,\n",
    "                                 pad_token_id_for_segment=0, pad_token_id_for_label=-100):\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "\n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        tokens = []\n",
    "        labels_ids = []\n",
    "        for one_word, label_token in zip(example, label):\n",
    "            subword_tokens = tokenizer.tokenize(one_word)\n",
    "            tokens.extend(subword_tokens)\n",
    "            labels_ids.extend([tag_to_index[label_token]]+ [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
    "\n",
    "\n",
    "        # CLS, SEP 토큰을 추가해야 하니까. 최대길이를 max_seq_len - 2로 맞춤\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "            labels_ids = labels_ids[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "        # SEP를 추가하는 코드, tokens에는 서브워드 된 하나의 문장에 대한 단어들이 요소임\n",
    "        tokens += [sep_token]\n",
    "        labels_ids += [pad_token_id_for_label] # 맨마지막(SEP)에 상응한는 -100을 추그\n",
    "        \n",
    "        \n",
    "        # CLS를 맨앞에 추가\n",
    "        tokens = [cls_token] + tokens\n",
    "        labels_ids = [pad_token_id_for_label] + labels_ids\n",
    "\n",
    "\n",
    "        # 정수인코딩\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # 어텐션 마스크 생성\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        \n",
    "        # 패딩시 정수인코딩 제외한 개수 \n",
    "        padding_count = max_seq_len - len(input_id)\n",
    "\n",
    "        # 정수인코딩을 제외한 나머지 패딩\n",
    "        input_id = input_id + ([pad_token_id] * padding_count)\n",
    "        \n",
    "        # 어텐션 마스크에도 정수(글자)외에 0으로 처리\n",
    "        attention_mask = attention_mask + ([0] * padding_count)\n",
    "        \n",
    "        # 세그먼트 인코딩\n",
    "        token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
    "        label = labels_ids + ([pad_token_id_for_label] * padding_count)\n",
    "\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cb0acba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81000/81000 [00:23<00:00, 3389.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터를 전처리 함\n",
    "X_train, y_train = convert_examples_to_features(train_data_sentence, train_data_label, max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9c596f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [00:02<00:00, 3550.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 전처리 함\n",
    "X_test, y_test = convert_examples_to_features(test_data_sentence, test_data_label, max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcd4837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 원문: ['정은', '씨를', '힘들게', '한', '가스나그,', '가만둘', '수', '없겠죠', '.']\n",
      "기존 레이블: ['PER-B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "--------------------------------------------------\n",
      "토큰화 후 원문: ['[CLS]', '정은', '씨', '##를', '힘들', '##게', '한', '가스', '##나', '##그', ',', '가만', '##둘', '수', '없', '##겠', '##죠', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "토큰화 후 라벨: ['[PAD]', 'PER-B', 'O', '[PAD]', 'O', '[PAD]', 'O', 'O', '[PAD]', '[PAD]', '[PAD]', 'O', '[PAD]', 'O', 'O', '[PAD]', '[PAD]', 'O', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "--------------------------------------------------\n",
      "정수 인코딩 결과: [    2 17915  1370  2138  4390  2318  1891  5809  2075  2029    16  6836\n",
      "  3056  1295  1415  2918  2321    18     3     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "정수 인코딩 라벨: [-100    1    0 -100    0 -100    0    0 -100 -100 -100    0 -100    0\n",
      "    0 -100 -100    0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100]\n"
     ]
    }
   ],
   "source": [
    "# 전처리를 샘플로 한번찍어보자\n",
    "print('기존 원문:', train_data_sentence[0])\n",
    "print('기존 레이블:', train_data_label[0])\n",
    "print('-' * 50)\n",
    "\n",
    "# X_train은 이미 서브토큰화 되어 정수 인코딩까지 되어 있는 상태임. 이것을 디코더화 해서 보여줌\n",
    "print('토큰화 후 원문:', [tokenizer.decode([word]) for word in X_train[0][0]])\n",
    "\n",
    "# 라벨\n",
    "print('토큰화 후 라벨:', ['[PAD]' if idx == -100 else index_to_tag[idx] for idx in y_train[0]])\n",
    "\n",
    "print('-' * 50)\n",
    "\n",
    "# 이것은 패딩까지 마친 상태임. X_train 정수인코딩, 세그먼트, 어텐션마스크까지 있다보니[][] 형태임\n",
    "print('정수 인코딩 결과:', X_train[0][0])\n",
    "print('정수 인코딩 라벨:', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd2f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어텐션 마스크: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "세그먼트: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 세그먼트와 어텐션도 출력해보자\n",
    "print('어텐션 마스크:', X_train[1][0])\n",
    "print('세그먼트:', X_train[2][0]) # 문장이 하나니까 모두 0 으로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c201c2a",
   "metadata": {},
   "source": [
    "모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7134729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링 클래스를 정의하자. 여기서는 활성화 함수 대신 손실함수에서 처리하도록 한다. 실무에서는 그렇게 하지 않겠지\n",
    "class TFBertForTokenClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(TFBertForTokenClassification, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, from_pt = True)\n",
    "        \n",
    "        # 아래는 결국 이웃풋 텐서플 포함한 레이어 또는 행렬로 볼 수 있는데 이것 자체가 하나의 객체로 보면 된다.\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels,\n",
    "                                                kernel_initializer=tf.keras. initializers.TruncatedNormal (0.02),\n",
    "                                                name='classifier')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask, token_type_ids = inputs # inputs은 세개의 층을 포함한다.\n",
    "        \n",
    "        # self.bert는 모델이다. bert의 개체명인식을 위한 모델이기 때문에 아래와 같이 인자를 넣어준다.\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        # 아래가 중요하다.\n",
    "        all_output = outputs[0] # outputs[1]은 many-to-one일때 사용한다. 모델의 결과가 그러니까 따지고 들 필요 없다.\n",
    "        prediction = self.classifier(all_output)\n",
    "        \n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ee8a1",
   "metadata": {},
   "source": [
    "손실함수를 정의하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c0d8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기가 중요한데. -100은 제거하자. 즉 서브워드된 단어들은 제거한다는 의미이다.\n",
    "# shape_list, reshape(-1,형태) 이 함수들은 따로 알아보자. 특히 reshape에 -1은 알아서 형태 맞게 재배열하라는 의미이다.\n",
    "# 이럴겨우 차원이 한차원 줄어듬...이 부분이 정확히 이해는 안됨.\n",
    "\n",
    "def compute_loss(labels, logits):\n",
    "\n",
    "  # 다중클래스에서 소프트맥스 함수를 사용하지 않을 경우 from_logits=True 이렇게 해줘야 한다네...참 규칙도 더럽게는 많네\n",
    "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "  \n",
    "  active_loss = tf.reshape(labels, (-1,)) != -100\n",
    "  reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\n",
    "  labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
    "\n",
    "  return loss_fn(labels, reduced_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae498b",
   "metadata": {},
   "source": [
    "학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7eed89cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user06\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.embeddings.position_ids', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForTokenClassification(\"klue/bert-base\", num_labels=tag_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ffc1eece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[    2,   942,  2747, ...,     0,     0,     0],\n",
       "        [    2,    25,  2252, ...,     0,     0,     0],\n",
       "        [    2,    17, 18012, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,  1443,  2394, ...,     0,     0,     0],\n",
       "        [    2,  1327,  2025, ...,     0,     0,     0],\n",
       "        [    2,  1675,  2043, ...,     0,     0,     0]]),\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "468dd7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음은 콜백함수를 재정의 합니다. 기존 함수를 써도 되는데 굳이 이렇게까지...\n",
    "# todo 이함수는 한번의 에포크가 끝날때마다 호출되어서 체크하게 되어 있습니다. 규칙이 그래..\n",
    "# todo 중요한 것은 정수시퀀스가 아니라 실제 태깅정보로 비교한다는 것\n",
    "\n",
    "class F1score(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    # 여기가 정수시퀀스가 아니라 실제 태깅정보로 변환. f1스코어를 구하기 위해서는 정수시퀀스로 안됨\n",
    "    def sequences_to_tags(self, label_ids, pred_ids):\n",
    "        label_list = []\n",
    "        pred_list = []\n",
    "        \n",
    "        for i in range(0, len(label_ids)):\n",
    "            label_tag = []\n",
    "            pred_tag =[]\n",
    "            \n",
    "            # 레이블 값이 -100은 것은 f1 score 계산 시에도 제외\n",
    "            for label_index, pred_index in zip(label_ids[i], pred_ids[i]):\n",
    "                if label_index != -100:\n",
    "                    label_tag.append(index_to_tag[label_index])\n",
    "                    pred_tag.append(index_to_tag[pred_index])\n",
    "                    \n",
    "            label_list.append(label_tag)\n",
    "            pred_list.append(pred_tag)\n",
    "            \n",
    "        return label_list, pred_list\n",
    "    \n",
    "    # 여기가 하나의 에포크가 끝날때마다 호출되는 함수. 콜백함수 상속을 받았기 때문에 규칙이야. 따질 필요 없음\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # 리턴값이 3차원 백터로 리턴 한갑네...1차원:한문장. 2차원:한단어 3차원:한단어에 대한 라벨값 배열 이렇게 생각하자 머리아프다\n",
    "        y_predicted = self.model.predict(self.X_test)  \n",
    "        y_predicted = np.argmax(y_predicted, axis = 2) # 3차원 배열에서 세번째 축에서 가장 큰 값의 인덱스 리턴\n",
    "        \n",
    "        label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n",
    "        score = f1_score(label_list, pred_list, suffix=True)\n",
    "        print(' - f1: {:04.2f}'.format(score * 100))\n",
    "        print(classification_report(label_list, pred_list, suffix=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "434bf767",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_report = F1score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4836ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, y_train, epochs=3, batch_size=32,\n",
    "    callbacks = [f1_score_report]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 문장에 대한 전처리 함수. 훈련시 차이점은 라벨이 존재하지 않는다는 것이다.\n",
    "def convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer,\n",
    "                                 pad_token_id_for_segment=0, pad_token_id_for_label=-100):\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids, label_masks = [], [], [], []\n",
    "\n",
    "    for example in tqdm(examples):\n",
    "        tokens = []\n",
    "        label_mask = []\n",
    "        for one_word in example:\n",
    "            \n",
    "            # 하나의 문장에 대한 서브워드 토큰화\n",
    "            subword_tokens = tokenizer.tokenize(one_word)\n",
    "            tokens.extend(subword_tokens)\n",
    "            \n",
    "            # 서브워드 중 첫번째 단어만 0으로 채우고 나머지는 -100으로 채운다.\n",
    "            label_mask.extend([0]+ [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
    "\n",
    "        # 전체길이중 cls,sep을 자리를 위해 2칸을 비운다.\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "            label_mask = label_mask[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "        # 토큰화 되어 있는 맨 마지막에 SEP을 추가\n",
    "        tokens += [sep_token]\n",
    "        label_mask += [pad_token_id_for_label]\n",
    "\n",
    "        # 토큰화 되어 있는 맨 앞에 CLS 토큰 추가\n",
    "        tokens = [cls_token] + tokens\n",
    "        \n",
    "        # 토큰되어 있는 맨 앞쪽에 -100추가, 여기는 라벨 마스크야.\n",
    "        label_mask = [pad_token_id_for_label] + label_mask\n",
    "\n",
    "        # 토큰화 되어 있는 것을 정수 인코딩\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # 어텐션 마스크\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        \n",
    "        # 패딩으로 채울 개수 산정\n",
    "        padding_count = max_seq_len - len(input_id)\n",
    "        \n",
    "        # 정수인코딩 된 것 뒤에 패딩토큰으로 채우기\n",
    "        input_id = input_id + ([pad_token_id] * padding_count)\n",
    "        \n",
    "        # 어텐션도 마찬가지로 나머지는 패팅토큰으로 채우기\n",
    "        attention_mask = attention_mask + ([0] * padding_count)\n",
    "\n",
    "        # 세그먼트 토큰\n",
    "        token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
    "        \n",
    "        # 라벨마스크에 패딩길이만큼 -100으로 채운다. 라벨마스크는 정수인코딩을 0, -100으로만 인코딩 함\n",
    "        label_mask = label_mask + ([pad_token_id_for_label] * padding_count)\n",
    "\n",
    "       \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        label_masks.append(label_mask)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    label_masks = np.asarray(label_masks, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), label_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a6c611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 714.65it/s]\n"
     ]
    }
   ],
   "source": [
    "X_pred, label_masks = convert_examples_to_features_for_prediction(test_data_sentence[:5], max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4e1325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 원문  : ['라티은-원윤정,', '휘닉스파크클래식', '프로골퍼']\n",
      "--------------------------------------------------\n",
      "토큰화 후 원 문 : ['[CLS]', '라', '##티', '##은', '-', '원', '##윤', '##정', ',', '휘', '##닉스', '##파크', '##클', '##래', '##식', '프로', '##골', '##퍼', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "레이블 마스크 : ['[PAD]', '[FIRST]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[FIRST]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[FIRST]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# 테스트 해보자\n",
    "print('기존 원문  :', test_data_sentence[0])\n",
    "print('-' * 50)\n",
    "print('토큰화 후 원 문 :', [tokenizer.decode([word]) for word in X_pred[0][0]])\n",
    "print('레이블 마스크 :', ['[PAD]' if idx == -100 else '[FIRST]' for idx in label_masks[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867afd5b",
   "metadata": {},
   "source": [
    "예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c6614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 함수\n",
    "def ner_prediction(examples, max_seq_len, tokenizer):\n",
    "    \n",
    "  examples = [sent.split() for sent in examples]\n",
    "  \n",
    "  # 전처리\n",
    "  X_pred, label_masks = convert_examples_to_features_for_prediction(examples, max_seq_len=128, tokenizer=tokenizer)\n",
    "  y_predicted = model.predict(X_pred)\n",
    "  \n",
    "  # 3차원으로 값을 리턴하니까\n",
    "  y_predicted = np.argmax(y_predicted, axis = 2)\n",
    "\n",
    "  pred_list = []\n",
    "  result_list = []\n",
    "\n",
    "  #-100 값을 삭제해서 디코딩헤서 pred_list에 추가\n",
    "  for i in range(0, len(label_masks)):\n",
    "    pred_tag = []\n",
    "    for label_index, pred_index in zip(label_masks[i], y_predicted[i]):\n",
    "      if label_index != -100:\n",
    "        pred_tag.append(index_to_tag[pred_index])\n",
    "\n",
    "    pred_list.append(pred_tag)\n",
    "\n",
    "  # 토큰화된 문장의 단어마다 개체명을 묶어서 반납\n",
    "  for example, pred in zip(examples, pred_list):\n",
    "    one_sample_result = []\n",
    "    for one_word, label_token in zip(example, pred):\n",
    "      one_sample_result.append((one_word, label_token))\n",
    "    result_list.append(one_sample_result)\n",
    "\n",
    "  return result_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
